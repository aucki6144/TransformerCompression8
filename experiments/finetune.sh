python run_finetuning.py \
        --model meta-llama/Llama-3.2-1B \
        --sliced-model-path ~/autodl-tmp/checkpoints_compressed/sliced-Llama-1B \
        --save-dir ~/autodl-tmp/checkpoints_compressed_tuned/sliced-Llama-1B-tuned \
        --sparsity 0.10 \
        --device cuda:0 \
        --ppl-eval-dataset alpaca \
        --ppl-eval-batch-size 1 \
        --finetune-dataset alpaca \
        --finetune-train-nsamples 2048 \
        --finetune-train-seqlen 512 \
        --finetune-train-batch-size 1 \
        --lora-alpha 10 \
        --lora-r 32 \
        --lora-dropout 0.1 \
        --lora-target-option attn_head_and_mlp \
        --eval-steps 16 \
        --save-steps 16 \
        --no-wandb